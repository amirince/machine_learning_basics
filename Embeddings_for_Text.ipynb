{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "copyright"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2021 Google LLC.\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxI782D_VLzp"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# Embeddings for Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43534tdfgs-v"
      },
      "source": [
        "In this lab, we'll train models for sentiment classification and experiment with learned embeddings for text features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY7emKinFRCr"
      },
      "source": [
        "### Learning Objectives\n",
        "* Understand the basics of text processing using variable-length inputs.\n",
        "* Understand *sparse* vs. *one-hot* representations.\n",
        "* Compare models that use one-hot representations with models that use learned embeddings.\n",
        "* Build intuition for what is learned in the embedding parameters.\n",
        "* Practice setting up models in Keras using layers for concatenation, averaging, embeddings, and fully-connected feed-forward connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gKL1mrkyLux"
      },
      "source": [
        "### Grading\n",
        "\n",
        "This assignment (exercises 1-5) are worth a total of 40 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "import plotly.graph_objs as plotly  # for interactive plots\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqppUDpmdptk"
      },
      "source": [
        "---\n",
        "## Data for Sentiment Classification\n",
        "\n",
        "In this lab, we'll train a *sentiment* classifier for movie reviews. That is, the input is the text of a movie review and the output is the probability the input was a positive review. The target labels are binary, 0 for negative and 1 for positive.\n",
        "\n",
        "Our data includes 50,000 movie reviews on IMDB. The data comes pre-segmented into train and test splits. The [data loading function](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data) below also splits each input text into tokens (words) and maps the words to integer values. Each input is a sequence of integers corresponding to the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6M-asvhQWV_"
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=None,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"Y_train.shape:\", Y_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"Y_test.shape:\", Y_test.shape)\n",
        "\n",
        "print('First training example data:', X_train[0])\n",
        "print('First training example label:', Y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyIWiy-4gQK-"
      },
      "source": [
        "So our first training example is a positive review. But that sequence of integer IDs is hard to read. The data loader provides a dictionary mapping words to IDs --- **let's create a reverse index to go from ID back to the corresponding word**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-qATkhUj7c"
      },
      "source": [
        "# The imdb dataset comes with an index mapping words to integers.\n",
        "# In the index the words are ordered by frequency they occur.\n",
        "index = imdb.get_word_index()\n",
        "\n",
        "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
        "# symbols, we need to add 3 to the index values.\n",
        "index = dict([(key, value + 3) for (key, value) in index.items()])\n",
        "\n",
        "# Create a reverse index (k, v) --> (v, k) so we can lookup tokens by their id.\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
        "\n",
        "# Add entries for special tokens.\n",
        "reverse_index[1] = '<START>'  # start of input\n",
        "reverse_index[2] = '#'        # out-of-vocabulary (OOV)\n",
        "reverse_index[3] = '<UNUSED>'\n",
        "\n",
        "max_id = max(reverse_index.keys())\n",
        "print('Largest ID:', max_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h76-b07ehWNQ"
      },
      "source": [
        "Note that our index (and reverse index) have over 88,000 tokens. That's quite a large vocabulary! Let's also write a decoding function for our data to stitch together the movie sentence corresponding to a sequence of IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjobmouHS5Dm"
      },
      "source": [
        "def decode(token_ids):\n",
        "  \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
        "  # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
        "  tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
        "\n",
        "  # Connect the string tokens with a space.\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# Show the ids corresponding to tokens in the first example.\n",
        "print(X_train[0])\n",
        "print(decode(X_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g47w5CackGBA"
      },
      "source": [
        "### Text Lengths\n",
        "As usual, let's start with some data analysis. How long are the reviews? Is there a difference in length between positive and negative reviews? A histogram will help answer these questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEOgzo8Gk3r7"
      },
      "source": [
        "# Create a list of lengths for training examples with a positive label.\n",
        "text_lengths_pos = [len(x) for (i, x) in enumerate(X_train) if Y_train[i]]\n",
        "\n",
        "# And a list of lengths for training examples with a negative label.\n",
        "text_lengths_neg = [len(x) for (i, x) in enumerate(X_train) if not Y_train[i]]\n",
        "\n",
        "# The histogram function can take a list of inputs and corresponding labels.\n",
        "plt.hist([text_lengths_pos, text_lengths_neg], bins=20, range=(0, 1000),\n",
        "         label=['positive', 'negative'])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Also check the longest reviews.\n",
        "print('Longest positive review:', max(text_lengths_pos))\n",
        "print('Longest negative review:', max(text_lengths_neg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ZE9gpkml3a"
      },
      "source": [
        "### Exercise 1: Token Counts (8 points)\n",
        "For each of the given tokens, construct a table with the number of positive training examples that include that token and the number of negative training examples that include that token. For reference, here are the counts for the first two tokens:\n",
        "\n",
        "|Token|Pos Count|Neg Count|\n",
        "|-|-|-|\n",
        "|good|4767|4849|\n",
        "|bad|1491|4396|\n",
        "\n",
        "You can copy this table and fill it in manually, or just print out the values via Python directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D6rGR1c1XQH"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YOYo6d01aWI"
      },
      "source": [
        "tokens = ['good', 'bad', 'amazing', 'boring', 'laugh', 'cry']\n",
        "# YOUR CODE HERE\n",
        "\n",
        "pos_rev = X_train[np.where(Y_train == 1)[0]]\n",
        "neg_rev = X_train[np.where(Y_train == 0)[0]]\n",
        "\n",
        "token_count = pd.DataFrame()\n",
        "token_count[\"Token\"] = tokens\n",
        "\n",
        "neg_count = []\n",
        "pos_count = []\n",
        "\n",
        "for token in tokens:\n",
        "  pos_count.append(sum(1 for sublist in pos_rev if index[token] in sublist))\n",
        "  neg_count.append(sum(1 for sublist in neg_rev if index[token] in sublist))\n",
        "\n",
        "token_count[\"Pos Count\"] = pos_count\n",
        "token_count[\"Neg Count\"] = neg_count\n",
        "\n",
        "token_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index['good'], decode([index['good']]))"
      ],
      "metadata": {
        "id": "vFNXfIYHKFJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhzt-LnQ1m8w"
      },
      "source": [
        "---\n",
        "## Feature Representation\n",
        "Consider the difference between the pixel features we used for image classification and the text features we are now dealing with.\n",
        "\n",
        "An image had 784 pixel positions. At each position, there is a single value in [0,1] (after normalization).\n",
        "\n",
        "In contrast, a review has a variable number of ordered tokens (up to 2494 in the training examples). Each token occurs in a particular position. We can think of the token positions much like the 784 pixel positions, except that some of the trailing positions are empty, since review lengths vary.  At each token position, there is a single token, one of the 88587 entries in the vocabulary. So we can think of a review as a (2500, 90000) matrix: At each of ~2500 token positions, we have 1 of ~90000 token ids.\n",
        "\n",
        "This representation would have 2500 * 90000 = 225 million features -- quite a lot more complexity than the images, though as you'll see below, we will make some simplifying assumptions, reducing both the number of token positions and the number of vocabulary items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm_F5JmWyfko"
      },
      "source": [
        "### Padding and Reduced Length\n",
        "As is clear from the length histogram, the current representation of the review text is a variable-length array. Since fixed-length arrays are easier to work with in Tensorflow, let's add special padding tokens at the end of each review until they are all the same length.\n",
        "\n",
        "We'll also use this operation to limit the number of token positions by truncating all reviews to a specified length. In the code below, as an example, we pad all training inputs to length 300."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4ou8bSUCWOx"
      },
      "source": [
        "def pad_data(sequences, max_length):\n",
        "  # Keras has a convenient utility for padding a sequence.\n",
        "  # Also make sure we get a numpy array rather than an array of lists.\n",
        "  return np.array(list(\n",
        "      tf.keras.preprocessing.sequence.pad_sequences(\n",
        "          sequences, maxlen=max_length, padding='post', value=0)))\n",
        "\n",
        "# Pad and truncate to 300 tokens.\n",
        "X_train_padded = pad_data(X_train, max_length=300)\n",
        "\n",
        "# Check the padded output.\n",
        "print('Length of X_train[0]:', len(X_train[0]))\n",
        "print('Length of X_train_padded[0]:', len(X_train_padded[0]))\n",
        "print(X_train_padded[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEmcwBjL4e_"
      },
      "source": [
        "### Reduced Vocabulary\n",
        "We also want to be able to limit the vocabulary size. Since our padding function produces fixed-length sequences in a numpy matrix, we can use clever numpy indexing to efficiently replace all token IDs larger than some value with the designated out-of-vocabulary (OOV) ID.\n",
        "\n",
        "In the code below, as an example, we'll keep just token IDs less than 1000, replacing all others with OOV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21qpyEgGNQeB"
      },
      "source": [
        "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
        "  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
        "  reduced_sequences = np.copy(sequences)\n",
        "  reduced_sequences[reduced_sequences >= max_token_id] = oov_id\n",
        "  return reduced_sequences\n",
        "\n",
        "# Reduce vocabulary to 1000 tokens.\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "print(X_train_reduced[0])\n",
        "\n",
        "# Decode to see what this looks like in tokens. Note the '#' for OOVs.\n",
        "print(decode(X_train_reduced[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d24mOPC6ybC4"
      },
      "source": [
        "### One-hot Encoding\n",
        "Our current feature representations are **sparse**. That is, we only keep track of the token IDs that are present in the input. A **one-hot** encoding replaces a value like 22 (corresponding to 'film') with an array with a single 1 at position 22 and zeros everywhere else. This will be very memory-inefficient, but we'll do it anyway for clarity.\n",
        "\n",
        "As discussed above, let's dramatically reduce both the number of token positions (review length) and the number of token ids (vocabulary). We'll clip each review after 20 tokens (so 2500 -> 20) and keep only the most common 1000 tokens (so 90000 -> 1000)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXzkqVL3Jufj"
      },
      "source": [
        "# Keras has a util to create one-hot encodings.\n",
        "X_train_padded = pad_data(X_train, max_length=20)\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
        "print('X_train_one_hot shape:', X_train_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5RvIN4w66Ej"
      },
      "source": [
        "Note the shape of the one-hot encoded features. For each of our 25000 training examples, we have a 20 x 1000 matrix. That is, for each of 20 token positions, we have a vector of 1000 elements containing a single 1 and 999 zeros.\n",
        "\n",
        "We can think of these 1000-dimensional one-hot arrays as **embeddings**. Each token in the input has a 1000-dimensional representation. But because of the one-hot setup, the distance between each pair of tokens is the same ([1,0,0,...], [0,1,0,...], etc.). By contrast, learned embeddings result in meaningful distances between pairs of tokens. We'll get to that soon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "296Cnt647b5c"
      },
      "source": [
        "---\n",
        "## Logistic Regression with One-Hot Encodings\n",
        "Let's start with something familiar -- logistic regression. Since our feature representation is in 2 dimensions (20 x 1000), we need to flatten it to pass it to Keras (remember we did this with the pixel data too). Let's try two strategies for flattening.\n",
        "\n",
        "1. Flatten by *concatenating* (as we did with pixels), turning (20 x 1000) data into (20000,) data. The result is a separate feature for each token at each position.\n",
        "2. Flatten by *averaging* over token positions, turning (20 x 1000) data into (1000,) data. The result is an array with average token counts, ignoring position.\n",
        "\n",
        "NOTE: Our prior assignments have used the standard Stochastic Gradient Descent (SGD) optimizer to compute the gradient from an estimate of the loss (based on the current mini-batch). There are many alternative optimizers. Here we'll use the [**Adam**](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimizer, which sometimes gives better results. One key characteristic of Adam is that it effectively uses a different learning rate for each parameter rather than a fixed learning rate as in SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m6eebM-0dUW"
      },
      "source": [
        "def build_onehot_model(average_over_positions=False):\n",
        "  \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  if average_over_positions:\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # sigmoid activation for classification\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',   # this is a classification task\n",
        "                optimizer='adam',             # fancy optimizer\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY3W_1-OSZ2X"
      },
      "source": [
        "Now let's try fitting the model to our training data and check performance metrics on the validation (held-out) data. But first, here's a function for plotting the learning curves given the training history object we get from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOVmajSuMjN6"
      },
      "source": [
        "def plot_history(history):\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
        "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
        "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyE4PgX70_op"
      },
      "source": [
        "model = build_onehot_model(average_over_positions= True)\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss\n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuCh9aQPv7F_"
      },
      "source": [
        "### Exercise 2: Comparing logistic regrerssion models (8 points)\n",
        "Train the one-hot model using both the concatenating and the averaging strategies and compare the results. You can switch between the models by changing the value of the parameter `average_over_positions` passed into `build_onehot_model()` --- by default, this is False.\n",
        "\n",
        "Let's call these *LR-C* (Logistic Regression Concatenating) and *LR-A* (Logistic Regression Averaging). Then answer the following questions:\n",
        "\n",
        "1. What are the final training and validation accuracies for LR-C and LR-A?\n",
        "2. How many parameters are there in each model (including bias)?\n",
        "3. Would you say that either model is overfitting? Why or why not?\n",
        "4. Briefly describe how LR-C differs from LR-A. What can you tell about the performance of the two models relative to each other by comparing the final validation accuracy of the two models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEAN5BejHc__"
      },
      "source": [
        "#### Student Solution\n",
        "\n",
        "1.\n",
        "\n",
        "|Model|Train Acc|Validation Acc|\n",
        "|--|--|--|\n",
        "|LR-C|0.8096|0.6860|\n",
        "|LR-A|0.6984|0.6924|\n",
        "\n",
        "2.\n",
        "\n",
        "LR-C: 20001\n",
        "\n",
        "LR-A: 1001\n",
        "\n",
        "3.\n",
        "For model LR-C we can deduce that there is some overfitting present since with the training data after 5 epochs, the loss is much smaller than it is with the test data. From the loss curve we can see the model performs better on the training data than the test data which indicates some form of overtfitting on the training data. Model LR-A performs pretty similarly on the train data and test data hence there is not probable cause to assume overfitting.\n",
        "\n",
        "4.\n",
        "LR-A Averages out, LR-C concatenates.\n",
        "Both models seem to perform pretty closely to each other on the validation data, within approximately 0.1 loss of each other after 5 epochs. LR-C has a higher training accuracy than LR-A however LR-C has a higher validation accuracy which implies that it performs better on unseen data than LR-A eventhough LR-C exhibits some overfitting. LR-C also has a lot more parameter than LR-A which could explain why LR-C is overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJIBRqK7lsjG"
      },
      "source": [
        "---\n",
        "## Logistic Regression with Embeddings\n",
        "Next, let's train model that replaces one-hot representations of each token with learned embeddings.\n",
        "\n",
        "The code below uses a Keras Embedding layer, which expects to receive a sparse (rather than one-hot) representation. That is, it expects a (padded) sequence of token IDs; for each ID, it looks up the corresponding embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho6uOeCaBs2e"
      },
      "source": [
        "def build_embeddings_model(average_over_positions=False,\n",
        "                           vocab_size=1000,\n",
        "                           sequence_length=20,\n",
        "                           embedding_dim=2):\n",
        "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      input_length=sequence_length)\n",
        "  )\n",
        "\n",
        "  if average_over_positions:\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyhoEjAiFSNB"
      },
      "source": [
        "Try training the model as before. We'll use the averaging strategy rather than the concatenating strategy for dealing with the token sequence. That is, we'll look up embedding vectors for each token. Then we'll average them to produce a single vector. Then we'll train a logistic regression with that vector as input to predict the binary label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYUE5UwkxoU8"
      },
      "source": [
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=2)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3k__61hFnag"
      },
      "source": [
        "### Exercise 3: Experiments with embeddings (8 points)\n",
        "Train 6 models with embedding dimensions in `[2, 4, 8, 16, 32, 64]`, keeping all other settings fixed. **Use the averaging strategy** rather than the concatenating strategy.\n",
        "\n",
        "1. Construct a table with the training and validation accuracies of each model (after 5 training epochs).\n",
        "2. Compute the number of parameters in each model.\n",
        "3. Do learned embeddings appear to provide improved performance over the one-hot encoding? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7t46ZdX2ofd"
      },
      "source": [
        "#### Student Solution\n",
        "\n",
        "1.\n",
        "\n",
        "|Embedding Size|Train Acc|Validation Acc|\n",
        "|-|-|-|\n",
        "|2|0.7211|0.7108|\n",
        "|4|0.7303|0.7184|\n",
        "|8|0.7400|0.7204|\n",
        "|16|0.7486|0.7276|\n",
        "|32|0.7517|0.7292|\n",
        "|64|0.7552|0.7284|\n",
        "\n",
        "2.\n",
        "\n",
        "|Embedding Size|Embedding Params|LR Params|Total|\n",
        "|-|-|-|-|\n",
        "|2|2000|3|2003|\n",
        "|4|4000|5|4005|\n",
        "|8|8000|9|8009|\n",
        "|16|16000|17|16017|\n",
        "|32|32000|33|32033|\n",
        "|64|64000|65|64065|\n",
        "\n",
        "3.\n",
        "\n",
        "Yes they seem to provide improved performance, they learned embeddings have a higher validation accuracy which implies better perfomance. the embeddings allow us to seprate positve and negative words which is one possible reason for the better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2dWOuxqKHA6"
      },
      "source": [
        "---\n",
        "## Inspecting Learned Embeddings\n",
        "Let's retrieve the learned embedding parameters from the trained model and plot the token embeddings.\n",
        "\n",
        "The model layers in a Keras Sequential model are stored as a list and the embeddings are the first layer. We can use the get_weights() function to get a numpy array with the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfsbGSwkaFjo"
      },
      "source": [
        "# Display the model layers.\n",
        "display(model.layers)\n",
        "\n",
        "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "display(embeddings.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apPWscNwcXTE"
      },
      "source": [
        "Now we'll use a fancy plotting tool called *`plotly`* to show the embeddings with hovertext so you can move your mouse over the points to see the corresponding tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RZMTrA0KttL"
      },
      "source": [
        "def plot_2d_embeddings(embeddings, id_start=1, count=100):\n",
        "  # Get 1st and 2nd embedding dims for the desired tokens.\n",
        "  x1 = embeddings[id_start:id_start+count, 0]\n",
        "  x2 = embeddings[id_start:id_start+count, 1]\n",
        "\n",
        "  # Get the corresponding words from the reverse index (for labeling).\n",
        "  tokens = [reverse_index[i] for i in range(id_start, id_start+count)]\n",
        "\n",
        "  # Plot with the plotly library.\n",
        "  data = plotly.Scatter(x=x1, y=x2, text=tokens,\n",
        "                        mode='markers', textposition='bottom left',\n",
        "                        hoverinfo='text')\n",
        "  fig = plotly.Figure(data=[data],\n",
        "                      layout=plotly.Layout(title=\"Word Embeddings\",\n",
        "                                           hovermode='closest'))\n",
        "  fig.show()\n",
        "\n",
        "# Very frequent tokens tend to be more syntactic than semantic, so let's plot\n",
        "# some rarer words.\n",
        "plot_2d_embeddings(embeddings, id_start=500, count=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Mm8MjRcZ20"
      },
      "source": [
        "### Exercise 4: Interpretting Embeddings (8 points)\n",
        "Notice that the 2-D embeddings fall in a narrow diagonal band.\n",
        "\n",
        "1. Have the learned embeddings separated positive and negative words? What is the most negative word? Does this make sense?\n",
        "2. Give 2 examples of words that seem to have surprising embedding values and try to explain their positions. For example, what's going on with the tokens '7', '8', and '9'?\n",
        "3. The embedding for 'crazy' is very close to (0,0). Explain what this means in terms of the model's output.\n",
        "4. Can you explain what you think the 2 learned embedding dimensions mean, if anything?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qAAvvo2y3t"
      },
      "source": [
        "#### Student Solution\n",
        "\n",
        "1.\n",
        "It seems as if the learned embeddings are separated into positve and negative words. The most negative word is \"avoid\". This makes sense since the model seems to be able to determine which words contribute to a review being positive or negative and it makes sense that avoid is the most negative word since it implies to not watch the movie at all which mean the comment implies the user did not like the movie at all.\n",
        "\n",
        "2.\n",
        "'7','8','9' have very high positive ratings, one reason could be is that the user is giving the movie a rating out of 10 which implies that their review of the film is highly positive. Also the token \"joke\" is associated with negative reviews, I would assume that if a comment contains the word jokes implies that the movie was good but here it implies that the movie was bad. The word joke could be used in phrases such as \"this movie was a joke\" which would imply a negative review. Additionaly the token \"difficult\" is slightly associated with positive reviews, this could imply that the word \"difficult\" was used in a positive context withint he reviews or it may be close to words like \"challenging,\" \"complex,\" or \"thought-provoking\" in thus the model might erroneously contribute to positive sentiment.\n",
        "\n",
        "3.\n",
        "crazy can be used to be an indicator of both good and bad review. So it is not much of an indicator wether the movies is good or bad since it can be used in multiple contexts. For instance in a positve review: \"That movie was absolutely crazy! The special effects were mind-blowing.\" and in a negative review: \"The pacing of the movie was crazy slow. It felt like nothing significant was happening for the first hour.\" Crazy having an embedding value of (0,0) implies that the words doesn't hold much influence as to wether a comment is negative or positive.\n",
        "\n",
        "\n",
        "4.\n",
        "They just add meaning to the words maybe the x-axis represents how close the words are to each other in meaning, y-axis shows the weight how much value it adds to wether the comment is negative or positiive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXCitmUvxfwb"
      },
      "source": [
        "---\n",
        "## Scaling Up!\n",
        "Remember how we limited our input sequences to 20 tokens and 1000 vocabulary entries? Let's see how well we can do using more data and bigger models (more parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKZDEGS7xzr6"
      },
      "source": [
        "### Exercise 5: Improve Results (8 points)\n",
        "Using pieces of code from above, set up and train a model that improves the validation accuracy to at least 80% (i.e., want >0.8). You should include the following elements:\n",
        "\n",
        "1. Truncate and pad input to the desired length.\n",
        "2. Limit vocabulary to the desired size.\n",
        "3. Set up a model using embeddings.\n",
        "4. Add an additional layer or layers (after the embeddings layer and before the output layer).\n",
        "5. Evaluate on the test data. Remember to apply the same pre-processing to the test data. You can use model.evaluate().\n",
        "\n",
        "Some scaffolding is provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xe8sRb2cCy"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekbJ4sIq2hID"
      },
      "source": [
        "def build_deep_embeddings_model(vocab_size,\n",
        "                                sequence_length,\n",
        "                                embedding_dim):\n",
        "\n",
        "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.keras.utils.set_random_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      input_length=sequence_length)\n",
        "  )\n",
        "\n",
        "\n",
        "  model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=4,\n",
        "      activation='relu'))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "X_train_padded = pad_data(X_train, max_length=600)\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=20000)\n",
        "\n",
        "model = build_deep_embeddings_model(vocab_size=20000,\n",
        "                               sequence_length=600,\n",
        "                               embedding_dim=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeGmywVmzLzX"
      },
      "source": [
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1,            # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDyQsMdKzdrB"
      },
      "source": [
        "X_test_padded = pad_data(X_test, max_length=600)\n",
        "X_test_reduced = limit_vocab(X_test_padded, max_token_id=20000)\n",
        "model.evaluate(X_test_reduced, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history"
      ],
      "metadata": {
        "id": "91GWtUva56XS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}